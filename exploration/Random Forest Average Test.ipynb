{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Independence Test v2 \n",
    "\n",
    "In this notebook, we modify our algorithm to average over H(Y|Xi) instead of averaging over trees. \n",
    "We will try out two methods: \n",
    "1. manually calculating the posterior distribution\n",
    "2. using random forest's approximation of the class probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have it so it splits training for you\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree\n",
    "import graphviz\n",
    "from scipy.stats import entropy\n",
    "#TODO: clean up code better\n",
    "#TODO: modularize and other stuff\n",
    "\n",
    "def estimate_conditional_entropy_hard_voting(X, y, n_trees = 10, max_depth = None, bootstrap = True):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    model = RandomForestClassifier(bootstrap = bootstrap, n_estimators =n_trees, max_depth = max_depth, random_state = 0)\n",
    "    model.fit(X_train, y_train)\n",
    "    print(X_test.flatten())\n",
    "    print(y_test)\n",
    "    class_counts = np.zeros((X_test.shape[0], model.n_classes_))\n",
    "    for tree_in_forest in model:\n",
    "        # get number of training elements in each partition\n",
    "        node_counts = tree_in_forest.tree_.n_node_samples\n",
    "        # get counts w.r.t. testing data now\n",
    "        partition_counts = np.asarray([node_counts[x] for x in tree_in_forest.apply(X_test)])\n",
    "        # get probability\n",
    "        class_probs = tree_in_forest.predict_proba(X_test)\n",
    "        # why are there decimals?!\n",
    "        # bootstrap approximation in sklearn\n",
    "        elems = np.multiply(class_probs, partition_counts[:, np.newaxis])\n",
    "        class_counts += elems\n",
    "    probs = class_counts/class_counts.sum(axis=1, keepdims=True)\n",
    "    entropies = -np.sum(np.log(probs)*probs, axis = 1)\n",
    "    return np.mean(entropies)\n",
    "\n",
    "def estimate_conditional_entropy_soft_voting(X, y, n_trees = 10, max_depth = None, bootstrap = True):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    model = RandomForestClassifier(n_estimators = n_trees, max_depth = max_depth, random_state = 0, bootstrap = bootstrap)\n",
    "    model.fit(X_train, y_train)\n",
    "    probs = model.predict_proba(X_test)\n",
    "    entropies = -np.sum(np.log(probs)*probs, axis = 1)\n",
    "    return np.mean(entropies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SKlearn Bootstrapping uses some weird approximation thing:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [0]*20 + [1]*20 + [2]*20 + [3]*20\n",
    "y = [0, 1, 0, 1, 0]*4 + [ 1, 1, 1, 1, 0]*4 + [1, 0, 1, 0, 1]*4 + [0, 0, 0, 0, 1]*4\n",
    "X = np.array(x).reshape(-1, 1)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hand calculations  \n",
    "H(X) = 1.386294  \n",
    "H(Y) = 0.693147  \n",
    "H(X, Y) = 1.97300  \n",
    "H(Y|X) = .5867  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 1 0 1 0 3 0 0 2 1 3 1 3 2 3 3 2 2 0 3 0 2]\n",
      "[1 0 1 1 1 1 0 0 0 0 1 1 0 1 0 1 0 0 1 1 0 1 0 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6143977929957609"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimate_conditional_entropy_hard_voting( X, y, 10, bootstrap = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing data Entropy calculation:\n",
    "H(X) = -(7/24log(7/24) + 6/24log(6/24) + 6/24log(6/24) + 5/24log(5/24)) = 1.397  \n",
    "H(y) = -(13/24log(13/24) + 11/24log(11/24)) = .68967  \n",
    "H(X, Y) = 6/24log(6/24) + 6/24log(6/24) + 1/24log(1/24) + 5/24log(5/24) + 1/24log(1/24) + 5/24log(5/24) = 1.61157  \n",
    "H(Y|X) =  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.619873263400197"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimate_conditional_entropy_soft_voting(X, y, 10, bootstrap = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can adjust the size of the data. The more data the better it does. However, it doesn't do as well as the previoius algorithm which uses weighted conditional entropy and first averages across trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improvement 1\n",
    "We can turn bootstrapping off because there is some approximation going on:\n",
    "https://stats.stackexchange.com/questions/130206/sklearn-tree-export-graphviz-values-do-not-add-up-to-samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.624708868541473"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimate_conditional_entropy_soft_voting( X, y, 100, bootstrap = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6247088685414731"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimate_conditional_entropy_hard_voting(X, y, 100, bootstrap = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improvement 2\n",
    "We can use all the data. This is by far where most of the error is coming from. Makes no sense to compare conditional entropy of test dataset to entire dataset. If we want to measure conditional entrop of our sample dataset, we should just use everything. What is important is just that random forest was able to capture dependences.\n",
    "\n",
    "How does this affect robustness? I.e. sample data is dependent but actually not dependent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual one\n",
    "def estimate_conditional_entropy_hard_voting(X, y, n_trees = 10, max_depth = None, bootstrap = True):\n",
    "    model = RandomForestClassifier(bootstrap = bootstrap, n_estimators =n_trees, max_depth = max_depth, random_state = 0)\n",
    "    model.fit(X, y)\n",
    "    class_counts = np.zeros((X.shape[0], model.n_classes_))\n",
    "    for tree_in_forest in model:\n",
    "        # get number of training elements in each partition\n",
    "        node_counts = tree_in_forest.tree_.n_node_samples\n",
    "        # get counts w.r.t. testing data now\n",
    "        partition_counts = np.asarray([node_counts[x] for x in tree_in_forest.apply(X)])\n",
    "        # get probability\n",
    "        class_probs = tree_in_forest.predict_proba(X)\n",
    "        # why are there decimals?!\n",
    "        class_counts += elems\n",
    "    probs = class_counts/class_counts.sum(axis=1, keepdims=True)\n",
    "    entropies = -np.sum(np.log(probs)*probs, axis = 1)\n",
    "    return np.mean(entropies)\n",
    "\n",
    "def estimate_conditional_entropy_soft_voting(X, y, n_trees = 10, max_depth = None, bootstrap = True):\n",
    "    model = RandomForestClassifier(n_estimators = n_trees, max_depth = max_depth, random_state = 0, bootstrap = bootstrap)\n",
    "    model.fit(X, y)\n",
    "    probs = model.predict_proba(X)\n",
    "    entropies = -np.sum(np.log(probs)*probs, axis = 1)\n",
    "    return np.mean(entropies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5867070452737222"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimate_conditional_entropy_soft_voting( X, y, 10, bootstrap = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6247088685414731"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimate_conditional_entropy_hard_voting(X, y, 10, bootstrap = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5747883706093118"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimate_conditional_entropy_soft_voting( X, y, 10, bootstrap = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5746704869940018"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimate_conditional_entropy_rf_hard_voting(X, y, 10, bootstrap = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "# take first 2 features\n",
    "X = iris.data[:100, :2]\n",
    "# take first 100 (only two classes)\n",
    "y = iris.target[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([], <a list of 0 Text yticklabel objects>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(X[:50, 1], X[:50, 0], c = \"b\", label = \"setosa\")\n",
    "plt.scatter(X[50:, 1], X[50:, 0], c = \"g\", label = \"vericolour\")\n",
    "plt.legend()\n",
    "plt.title('Iris Dataset Scatter Plot')\n",
    "plt.xticks([])\n",
    "plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H(Y): 0.6931471805599453\n",
      "Conditional Entropy: nan\n",
      "Mutual Information: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/jupyter/1.0.0_5/libexec/lib/python3.7/site-packages/ipykernel_launcher.py:18: RuntimeWarning: divide by zero encountered in log\n",
      "/usr/local/Cellar/jupyter/1.0.0_5/libexec/lib/python3.7/site-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "\n",
    "def calculate_entropy(X):\n",
    "    probs = np.bincount(X)/len(X)\n",
    "    return scipy.stats.entropy(probs)\n",
    "entropy = calculate_entropy(y)\n",
    "cond_entropy = estimate_conditional_entropy_hard_voting(X, y, 10, bootstrap = False)\n",
    "print(\"H(Y):\", entropy)\n",
    "print(\"Conditional Entropy:\", cond_entropy)\n",
    "print(\"Mutual Information:\", entropy - cond_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

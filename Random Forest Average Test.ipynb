{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Independence Test v2 \n",
    "\n",
    "In this notebook, we modify our algorithm to average over H(Y|Xi) instead of averaging over trees. \n",
    "We will try out two methods: \n",
    "1. manually calculating the posterior distribution\n",
    "2. using random forest's approximation of the class probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have it so it splits training for you\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree\n",
    "import graphviz\n",
    "from scipy.stats import entropy\n",
    "#TODO: clean up code better\n",
    "#TODO: modularize and other stuff\n",
    "\n",
    "# manual one\n",
    "def estimate_conditional_entropy(X, y, n_trees = 10, max_depth = None, bootstrap = True):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    model = RandomForestClassifier(bootstrap = bootstrap, n_estimators =n_trees, max_depth = max_depth, random_state = 0)\n",
    "    model.fit(X_train, y_train)\n",
    "    class_counts = np.zeros((X_test.shape[0], model.n_classes_))\n",
    "    for tree_in_forest in model:\n",
    "        # get number of training elements in each partition\n",
    "        node_counts = tree_in_forest.tree_.n_node_samples\n",
    "        # get counts w.r.t. testing data now\n",
    "        partition_counts = np.asarray([node_counts[x] for x in tree_in_forest.apply(X_test)])\n",
    "        # get probability\n",
    "        class_probs = tree_in_forest.predict_proba(X_test)\n",
    "        # why are there decimals?!\n",
    "        # bootstrap approximation in sklearn\n",
    "        elems = np.multiply(class_probs, partition_counts[:, np.newaxis])\n",
    "        class_counts += elems\n",
    "    probs = class_counts/class_counts.sum(axis=1, keepdims=True)\n",
    "    entropies = -np.sum(np.log(probs)*probs, axis = 1)\n",
    "    return np.mean(entropies)\n",
    "\n",
    "def estimate_conditional_entropy_rf(X, y, n_trees = 10, max_depth = None, bootstrap = True):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    model = RandomForestClassifier(n_estimators = n_trees, max_depth = max_depth, random_state = 0, bootstrap = bootstrap)\n",
    "    model.fit(X_train, y_train)\n",
    "    probs = model.predict_proba(X_test)\n",
    "    entropies = -np.sum(np.log(probs)*probs, axis = 1)\n",
    "    return np.mean(entropies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SKlearn Bootstrapping uses some weird approximation thing:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [0]*320 + [1]*320 + [2]*320 + [3]*320\n",
    "y = [0, 1, 0, 1, 0]*64 + [ 1, 1, 1, 1, 0]*64 + [1, 0, 1, 0, 1]*64 + [0, 0, 0, 0, 1]*64\n",
    "X = np.array(x).reshape(-1, 1)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hand calculations  \n",
    "H(X) = 1.386294  \n",
    "H(Y) = 0.693147  \n",
    "H(X, Y) = 1.97300  \n",
    "H(Y|X) = .5867  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5902739280344956"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimate_conditional_entropy( X, y, 100, bootstrap = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can adjust the size of the data. The more data the better it does. However, it doesn't do as well as the previoius algorithm which uses weighted conditional entropy and first averages across trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5902189569467394"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimate_conditional_entropy_rf(X, y, 100, bootstrap = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improvement 1\n",
    "We can turn bootstrapping off because there is some approximation going on:\n",
    "https://stats.stackexchange.com/questions/130206/sklearn-tree-export-graphviz-values-do-not-add-up-to-samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5888136355677847"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimate_conditional_entropy( X, y, 100, bootstrap = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5888136355677847"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimate_conditional_entropy_rf(X, y, 100, bootstrap = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improvement 2\n",
    "We can use all the data. This is by far where most of the error is coming from. Makes no sense to compare conditional entropy of test dataset to entire dataset. If we want to measure conditional entrop of our sample dataset, we should just use everything. What is important is just that random forest was able to capture dependences.\n",
    "\n",
    "How does this affect robustness? I.e. sample data is dependent but actually not dependent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual one\n",
    "def estimate_conditional_entropy(X, y, n_trees = 10, max_depth = None, bootstrap = True):\n",
    "    model = RandomForestClassifier(bootstrap = bootstrap, n_estimators =n_trees, max_depth = max_depth, random_state = 0)\n",
    "    model.fit(X, y)\n",
    "    class_counts = np.zeros((X, model.n_classes_))\n",
    "    for tree_in_forest in model:\n",
    "        # get number of training elements in each partition\n",
    "        node_counts = tree_in_forest.tree_.n_node_samples\n",
    "        # get counts w.r.t. testing data now\n",
    "        partition_counts = np.asarray([node_counts[x] for x in tree_in_forest.apply(X)])\n",
    "        # get probability\n",
    "        class_probs = tree_in_forest.predict_proba(X)\n",
    "        # why are there decimals?!\n",
    "        # bootstrap approximation in sklearn\n",
    "        elems = np.multiply(class_probs, partition_counts[:, np.newaxis])\n",
    "        class_counts += elems\n",
    "    probs = class_counts/class_counts.sum(axis=1, keepdims=True)\n",
    "    entropies = -np.sum(np.log(probs)*probs, axis = 1)\n",
    "    return np.mean(entropies)\n",
    "\n",
    "def estimate_conditional_entropy_rf(X, y, n_trees = 10, max_depth = None, bootstrap = True):\n",
    "    model = RandomForestClassifier(n_estimators = n_trees, max_depth = max_depth, random_state = 0, bootstrap = bootstrap)\n",
    "    model.fit(X, y)\n",
    "    probs = model.predict_proba(X)\n",
    "    entropies = -np.sum(np.log(probs)*probs, axis = 1)\n",
    "    return np.mean(entropies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mutual Information Truths\n",
    "\n",
    "## Normal mutual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8f0676416654>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeans_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcovariances_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mestimate_x_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mnormal_entropy_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "from scipy.stats import multivariate_normal\n",
    "import scipy.integrate as integrate\n",
    "import math\n",
    "from sklearn import mixture\n",
    "\n",
    "def estimate_p(y):\n",
    "    return y.count(1)/len(y)\n",
    "\n",
    "def estimate_x_params(X):\n",
    "    model = mixture.GaussianMixture(n_components = 2, covariance_type = \"full\")\n",
    "    model.fit(X)\n",
    "    return model.means_, model.covariances_\n",
    "    \n",
    "estimate_x_params(X)\n",
    "\n",
    "def normal_entropy_f(t, mean, var):\n",
    "    return -norm.pdf(t, mean, var)*np.log(norm.pdf(t, mean, var))\n",
    "\n",
    "def two_mixture_normals_entropy_f(t, mean_1, mean_2, var_1, var_2):\n",
    "    return -.5*norm.pdf(t, mean_1, var_1)*np.log(.5*norm.pdf(t, mean_1, var_1) + .5*norm.pdf(t, mean_2, var_2)) - .5*norm.pdf(t, mean_1, var_1)*np.log(.5*norm.pdf(t, mean_1, var_1) + .5*norm.pdf(t, mean_2, var_2))\n",
    "\n",
    "def normal_entropy(var):\n",
    "    return .5*np.log(2*math.pi*math.e*var)\n",
    "\n",
    "#NOTE: this doesn't work for mean = 0\n",
    "def plugin_estimate_cat_1D(X, y):\n",
    "    y_param = estimate_p(y)\n",
    "    x_params = estimate_x_params(X)\n",
    "    h_y = -y_param*np.log(y_param) - (1 - y_param)*np.log(1 - y_param)\n",
    "    #h_x_cond_y = integrate.quad(normal_entropy_f, -20, 20, args = (x_params[0][0], x_params[1][0].item()))[0]*.5 + \\\n",
    "    #integrate.quad(normal_entropy_f, -20, 20, args = (x_params[0][1], x_params[1][1].item()))[0]*.5\n",
    "    h_x_cond_y = normal_entropy(x_params[1][0])*.5 + normal_entropy(x_params[1][1])*.5\n",
    "    h_x = integrate.quad(two_mixture_normals_entropy_f, -20, 20, args = (x_params[0][0], x_params[0][1], x_params[1][0].item(), x_params[1][1].item()))[0]\n",
    "    cond_entropy =  h_y - h_x + h_x_cond_y\n",
    "    if cond_entropy < 0:\n",
    "        return np.array((0))\n",
    "    return cond_entropy\n",
    "\n",
    "def truth_1d(mean):\n",
    "    h_y = -.5*np.log(.5) - .5*np.log(.5)\n",
    "    h_x_cond_y = normal_entropy(1)*.5 + normal_entropy(1)*.5\n",
    "    h_x = integrate.quad(two_mixture_normals_entropy_f, -20, 20, args = (mean, -mean, 1, 1))[0]\n",
    "    cond_entropy =  h_y - h_x + h_x_cond_y\n",
    "    if cond_entropy < 0:\n",
    "        return np.array((0))\n",
    "    return cond_entropy\n",
    "\n",
    "\n",
    "def mutual_information_truth():\n",
    "    entropy = -.5*np.log(.5) - .5*np.log(.5)\n",
    "    means = [i*-.2 for i in range(0, 26)]\n",
    "    means.reverse()\n",
    "    means.extend([i*.2 for i in range(1, 26)])\n",
    "    truths = []\n",
    "    for elem in means:\n",
    "        truths.append(entropy - truth_1d(elem))\n",
    "    #return means, cef_all, plugin_all, truth\n",
    "    return truths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different variances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "from scipy.stats import multivariate_normal\n",
    "import scipy.integrate as integrate\n",
    "import math\n",
    "from sklearn import mixture\n",
    "\n",
    "def estimate_p(y):\n",
    "    return y.count(1)/len(y)\n",
    "\n",
    "def estimate_x_params(X):\n",
    "    model = mixture.GaussianMixture(n_components = 2, covariance_type = \"full\")\n",
    "    model.fit(X)\n",
    "    return model.means_, model.covariances_\n",
    "    \n",
    "def normal_entropy_f(t, mean, var):\n",
    "    return -norm.pdf(t, mean, var)*np.log(norm.pdf(t, mean, var))\n",
    "\n",
    "def two_mixture_normals_entropy_f(t, mean_1, mean_2, var_1, var_2):\n",
    "    return -.5*norm.pdf(t, mean_1, var_1)*np.log(.5*norm.pdf(t, mean_1, var_1) + .5*norm.pdf(t, mean_2, var_2)) - .5*norm.pdf(t, mean_1, var_1)*np.log(.5*norm.pdf(t, mean_1, var_1) + .5*norm.pdf(t, mean_2, var_2))\n",
    "\n",
    "def normal_entropy(var):\n",
    "    return .5*np.log(2*math.pi*math.e*var)\n",
    "\n",
    "#NOTE: this doesn't work for mean = 0\n",
    "def plugin_estimate_cat_1D(X, y):\n",
    "    y_param = estimate_p(y)\n",
    "    x_params = estimate_x_params(X)\n",
    "    h_y = -y_param*np.log(y_param) - (1 - y_param)*np.log(1 - y_param)\n",
    "    #h_x_cond_y = integrate.quad(normal_entropy_f, -20, 20, args = (x_params[0][0], x_params[1][0].item()))[0]*.5 + \\\n",
    "    #integrate.quad(normal_entropy_f, -20, 20, args = (x_params[0][1], x_params[1][1].item()))[0]*.5\n",
    "    h_x_cond_y = normal_entropy(x_params[1][0])*.5 + normal_entropy(x_params[1][1])*.5\n",
    "    h_x = integrate.quad(two_mixture_normals_entropy_f, -20, 20, args = (x_params[0][0], x_params[0][1], x_params[1][0].item(), x_params[1][1].item()))[0]\n",
    "    cond_entropy =  h_y - h_x + h_x_cond_y\n",
    "    if cond_entropy < 0:\n",
    "        return np.array((0))\n",
    "    return cond_entropy\n",
    "\n",
    "def truth_1d(mean):\n",
    "    h_y = -.5*np.log(.5) - .5*np.log(.5)\n",
    "    h_x_cond_y = normal_entropy(1)*.5 + normal_entropy(3)*.5\n",
    "    h_x = integrate.quad(two_mixture_normals_entropy_f, -20, 20, args = (mean, -mean, 1, math.sqrt(3)))[0]\n",
    "    cond_entropy =  h_y - h_x + h_x_cond_y\n",
    "    if cond_entropy < 0:\n",
    "        return np.array((0))\n",
    "    return cond_entropy\n",
    "\n",
    "\n",
    "def mutual_information_truth():\n",
    "    entropy = -.5*np.log(.5) - .5*np.log(.5)\n",
    "    means=[i*.2 for i in range(0, 26)]\n",
    "    truths = []\n",
    "    for elem in means:\n",
    "        mi = entropy - truth_1d(elem)\n",
    "        if mi < 0:\n",
    "            truths.append(0)\n",
    "        else:\n",
    "            truths.append((entropy - truth_1d(elem))/entropy)\n",
    "    #return means, cef_all, plugin_all, truth\n",
    "    return truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "truths_diff_covar = mutual_information_truth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0.00139387239137524, 0.0923354486255463, 0.17983487386233699, 0.26014481159701247, 0.3311346125243729, 0.39191714540163064, 0.4424935478389682, 0.4834711645708754, 0.5158388832981347, 0.5407855870838623, 0.5595579047838196, 0.5733562865659112, 0.5832668885594593, 0.5902244414373682, 0.5949996896546803, 0.5982044676085582, 0.6003078952265687, 0.6016582208033919, 0.602506180664179, 0.6030271057430278, 0.6033401961717422]\n"
     ]
    }
   ],
   "source": [
    "print(truths_diff_covar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "from scipy.stats import multivariate_normal\n",
    "import scipy.integrate as integrate\n",
    "import math\n",
    "from sklearn import mixture\n",
    "\n",
    "def estimate_p(y):\n",
    "    return y.count(1)/len(y)\n",
    "\n",
    "def estimate_x_params(X):\n",
    "    model = mixture.GaussianMixture(n_components = 2, covariance_type = \"full\")\n",
    "    model.fit(X)\n",
    "    return model.means_, model.covariances_\n",
    "    \n",
    "\n",
    "def normal_entropy_f(t, mean, var):\n",
    "    return -norm.pdf(t, mean, var)*np.log(norm.pdf(t, mean, var))\n",
    "\n",
    "def two_mixture_normals_entropy_f(t, mean_1, mean_2, var_1, var_2, prob):\n",
    "    return -prob*norm.pdf(t, mean_1, var_1)*np.log(prob*norm.pdf(t, mean_1, var_1) + (prob)*norm.pdf(t, mean_2, var_2)) - (1-prob)*norm.pdf(t, mean_1, var_1)*np.log((1-prob)*norm.pdf(t, mean_1, var_1) + (1-prob)*norm.pdf(t, mean_2, var_2))\n",
    "\n",
    "def normal_entropy(var):\n",
    "    return .5*np.log(2*math.pi*math.e*var)\n",
    "\n",
    "#NOTE: this doesn't work for mean = 0\n",
    "def plugin_estimate_cat_1D(X, y):\n",
    "    y_param = estimate_p(y)\n",
    "    x_params = estimate_x_params(X)\n",
    "    h_y = -y_param*np.log(y_param) - (1 - y_param)*np.log(1 - y_param)\n",
    "    #h_x_cond_y = integrate.quad(normal_entropy_f, -20, 20, args = (x_params[0][0], x_params[1][0].item()))[0]*.5 + \\\n",
    "    #integrate.quad(normal_entropy_f, -20, 20, args = (x_params[0][1], x_params[1][1].item()))[0]*.5\n",
    "    h_x_cond_y = normal_entropy(x_params[1][0])*.5 + normal_entropy(x_params[1][1])*.5\n",
    "    h_x = integrate.quad(two_mixture_normals_entropy_f, -20, 20, args = (x_params[0][0], x_params[0][1], x_params[1][0].item(), x_params[1][1].item()))[0]\n",
    "    cond_entropy =  h_y - h_x + h_x_cond_y\n",
    "    if cond_entropy < 0:\n",
    "        return np.array((0))\n",
    "    return cond_entropy\n",
    "\n",
    "def truth_1d(prob):\n",
    "    h_y = -prob*np.log(prob) - (1-prob)*np.log(1-prob)\n",
    "    h_x_cond_y = normal_entropy(1)*prob + normal_entropy(1)*(1-prob)\n",
    "    h_x = integrate.quad(two_mixture_normals_entropy_f, -20, 20, args = (1, -1, 1, 1, prob))[0]\n",
    "    cond_entropy =  h_y - h_x + h_x_cond_y\n",
    "    if cond_entropy < 0:\n",
    "        return np.array((0))\n",
    "    return cond_entropy\n",
    "\n",
    "\n",
    "def mutual_information_truth():\n",
    "    probs = [i*.02 for i in range(1, 26)]\n",
    "    truths = []\n",
    "    for elem in probs:\n",
    "        entropy = -elem*np.log(elem) - (1-elem)*np.log(1 - elem)\n",
    "        truths.append(entropy - truth_1d(elem))\n",
    "    #return means, cef_all, plugin_all, truth\n",
    "    return truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "truths=mutual_information_truth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.2582772469333816,\n",
       " -0.18837221247894043,\n",
       " -0.12934883771250916,\n",
       " -0.07754698844452623,\n",
       " -0.031233386821665654,\n",
       " 0.010608631059595974,\n",
       " 0.048647124850824675,\n",
       " 0.08335351918822936,\n",
       " 0.11507724880731518,\n",
       " 0.14408606332507468,\n",
       " 0.17059160121826666,\n",
       " 0.19476356787385918,\n",
       " 0.21674055691830674,\n",
       " 0.2366369572343613,\n",
       " 0.25454794184177987,\n",
       " 0.27055309735931277,\n",
       " 0.28471911766804203,\n",
       " 0.29710183458058814,\n",
       " 0.30774776635099443,\n",
       " 0.3166953067961429,\n",
       " 0.32397563997903966,\n",
       " 0.3296134400392592,\n",
       " 0.3336273982452863,\n",
       " 0.3360306068768478,\n",
       " 0.3368308203468321]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[elem for elem in truths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

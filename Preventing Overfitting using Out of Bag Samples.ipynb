{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preventing Overfitting using Out-of-Bag Samples\n",
    "In this notebook, we will show that we are correctly using out of bag samples for sklearn's random forest implementation. Furthermore, we will show that this method was not affective in correctly estimating conditional entropy.\n",
    "\n",
    "We first use an extremely simple dataset just to show that we are correctly using out-of-bag for the decision tree classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [0, 1, 2, 3, 4, 5]\n",
    "y = [0, 0, 0, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our estimator, we are using Sklearn's Bagging Classifier on Decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [2],\n",
       "       [3],\n",
       "       [4],\n",
       "       [5]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble.forest import _generate_unsampled_indices\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "\n",
    "X = np.array(X).reshape(-1, 1)\n",
    "model = BaggingClassifier(DecisionTreeClassifier(), \n",
    "                              n_estimators = 2, \n",
    "                              max_samples= .5, \n",
    "                              bootstrap = True)\n",
    "model.fit(X, y)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's select the first classifier and check to see that we can correctly access the out of bag samples. To do so, we use this function called \"_generate_unsampled_indices\" that gets the unused indices in X. There's no good way of verifying that they are indeed unused, since the classifier does not store the in-bag samples. However, we can do the following tests max_samples = 1. (use all), max_samples = 1 (only use one) to do some quick validation. \n",
    "\n",
    "Let's first access a single classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 3 5]\n"
     ]
    }
   ],
   "source": [
    "classifier = model[0]\n",
    "unsampled_indices = _generate_unsampled_indices(classifier.random_state, len(X))\n",
    "print(unsampled_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 5]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "model = BaggingClassifier(DecisionTreeClassifier(), \n",
    "                              n_estimators = 2, \n",
    "                              max_samples= 0.2, \n",
    "                              bootstrap = False)\n",
    "model.fit(X, y)\n",
    "tree = model[0]\n",
    "unsampled_indices = _generate_unsampled_indices(tree.random_state, X.shape[0])\n",
    "print(unsampled_indices)\n",
    "node_counts = tree.tree_.n_node_samples\n",
    "print(node_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
